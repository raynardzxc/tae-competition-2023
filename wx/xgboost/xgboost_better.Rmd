---
title: "XGBoost"
output: html_notebook
---

# Load packages

The xgboost package contains the XGBoost algorithm and associated tools. You can view the formal documentation online: https://cran.r-project.org/web/packages/xgboost/xgboost.pdf.
```{r}
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages 
pkgnames <- c("dplyr","xgboost","splitTools", "caret")
# Use our custom load function
loadPkgs(pkgnames)
```

## Import Data
```{r}
totrain <- read.csv("train1_onehot2.csv")
pure <- totrain
topredict <- read.csv("test1_onehot2.csv")
```

## Label conversion

XGBoost requires the classes to be in an integer format, starting with 0. So, the first class should be 0. The Choice factor is converted to the proper integer format.
```{r}
# Convert the Choices factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
totrain$Choice <- as.factor(totrain$Choice)
choices <- totrain$Choice
label <- as.integer(totrain$Choice)-1
totrain$Choice <- NULL
totrain <- subset(totrain, select = -c(Case, No))
totrain <- subset(totrain, select = -c(CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
```

## Split the data for training and testing 
```{r}
set.seed(123)
inds <- createDataPartition(pure$Choice, p = 0.8, list = FALSE)
train.data = as.matrix(totrain[inds, ])
train.label = label[inds]
test.data = as.matrix(totrain[-inds, ])
test.label = label[-inds]

tempno <- pure$No[-inds]
tempchoice <- pure$Choice[-inds]

tempfullno <- pure$No
tempfullchoice <- pure$Choice
tempfulltrain <- totrain
```

<!-- ## Split the data for training and testing (OUTDATED) -->
<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- inds <- partition(totrain$Task, p = c(train = 0.8, test = 0.2)) -->
<!-- str(inds) -->
<!-- train.data = as.matrix(totrain[inds$train, ]) -->
<!-- train.label = label[inds$train] -->
<!-- test.data = as.matrix(totrain[inds$test, ]) -->
<!-- test.label = label[inds$test] -->
<!-- ``` -->

# Create the xgb.DMatrix objects

Next, we transform the training and testing data sets into xgb.DMatrix objects that are used for fitting the XGBoost model and predicting new outcomes.

```{r}
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
xgb.fulltrain = xgb.DMatrix(data=as.matrix(totrain),label=label)
```

## Define grid of hyperparameters to try
```{r}
grid <- expand.grid(nrounds = c(50, 100, 150), ## times the algo add a new decision tree to the ensemble model, higher more complex
                    max_depth = c(5),
                    eta = c(0.01, 0.1, 0.3), ## steps taken when updating the weights, higher faster
                    gamma = c(0, 2, 4), ## minimum loss reduction required to further partition, higher more simple
                    colsample_bytree = c(0.3, 0.5, 0.7, 0.9, 1), ## what fraction of features are used to train each tree, higher means all
                    min_child_weight = c(1, 3, 5), ## decides if new child will be better, higher more simple
                    subsample = c(0.6, 0.7, 0.8)) ## what fraction of train data are used to train each tree

ctrl <- trainControl(method = "cv", number = 5, verboseIter = FALSE)
```

# Train the model
```{r}
# Train the XGBoost classifer
xgb.fit <- train(x = train.data,
                 y = train.label,
                 method = "xgbTree",
                 trControl = ctrl,
                 tuneGrid = grid,
                 verbose = FALSE)

# Review the final model and results
xgb.fit
```

# Define the main parameters
```{r}
num_class = length(levels(choices))
params = list(
  eta=0.1, ## lower implies larger nrounds, means more robust to overfitting but slower to compute
  max_depth=5,
  gamma=0, ## pruning var, larger more conservative
  subsample=0.8,
  colsample_bytree=1,
  min_child_weight=3,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class
)

xgb.newfit=xgb.train(
  params=params,
  data=xgb.train,
  nrounds=150,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
)

xgb.pred = predict(xgb.newfit,xgb.test,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(choices)

xgb.fullpred = predict(xgb.newfit,xgb.fulltrain,reshape=T)
xgb.fullpred = as.data.frame(xgb.fullpred)
colnames(xgb.fullpred) = levels(choices)
```

## Check logloss
```{r}
## add back 1 for the testing data's Choice
test.label <- test.label+1
test.label <- as.data.frame(test.label)
colnames(test.label) <- c("Choice")

logloss <- function(test_set, testpredict_df) {
  # Create one-hot encoding for each choice on-the-fly
  Ch1 <- as.integer(test_set$Choice == 1)
  Ch2 <- as.integer(test_set$Choice == 2)
  Ch3 <- as.integer(test_set$Choice == 3)
  Ch4 <- as.integer(test_set$Choice == 4)
  
  # Calculate logloss using these one-hot encoded variables
  result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
                                    Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
                                    Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
                                    Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
  return(result)
}
colnames(xgb.pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss(test.label, xgb.pred)

colnames(xgb.fullpred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
templabel <- label + 1
templabel <- as.data.frame(templabel)
colnames(templabel) <- c("Choice")
logloss(templabel, xgb.fullpred)
```
## Store predictions of test set
```{r}
## export testresults
xgb.pred$No <- tempno
xgb.pred$Choice <- tempchoice
xgb.pred <- xgb.pred[c("No", "Ch1", "Ch2", "Ch3", "Ch4", "Choice")]

write.csv(xgb.pred, file = "testresults_xgb3.csv", row.names = FALSE)

## export testresults on full training set
xgb.fullpred$No <- tempfullno
xgb.fullpred$Choice <- tempfullchoice
xgb.fullpred <- xgb.fullpred[c("No", "Ch1", "Ch2", "Ch3", "Ch4", "Choice")]

write.csv(xgb.fullpred, file = "fulltestresults_xgb3.csv", row.names = FALSE)
```

## Make prediction
```{r}
## preparing data for prediction
predict_set <- subset(topredict, select = -c(Choice))
predict_set <- subset(predict_set, select = -c(Case, No))
predict_set <- subset(predict_set, select = -c(CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
predict_set <- as.matrix(predict_set)

## making prediction
xgb.finalpred = predict(xgb.newfit,predict_set,reshape=T)

## preparing prediction for export
predicted_df <- as.data.frame(xgb.finalpred)

colnames(predicted_df) <- c("Ch1", "Ch2", "Ch3", "Ch4")
predicted_df$No <- topredict$No
predicted_df <- predicted_df[c("No", "Ch1", "Ch2", "Ch3", "Ch4")]

## export
write.csv(predicted_df, file = "submission_xgb3.csv", row.names = FALSE)
```

