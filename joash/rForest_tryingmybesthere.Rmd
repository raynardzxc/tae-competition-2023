---
title: "Random Forest"
output: html_notebook
---

```{r}
rm(list=ls())
library(randomForest)
library(caret)
library(mlr)
library(data.table)
```

```{r}
library(parallelMap)
library(parallel)
parallelStartSocket(cpus = detectCores())
```

```{r}
safety <- read.csv("train1_trying.csv")
safety <- subset(safety, select=-c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))

# Define the breaks for the bins
#breaks <- c(-1, 0, 29999, 39999, 49999, 59999, 69999, 79999, 89999, 99999, 109999, 119999, 129999, 139999, 149999, 159999, 169999, 179999, 189999, 199999, 219999, 239999, 259999, 279999, 299999, Inf)

# Define the labels for the bins
#labels <- 1:25

# Convert the incomea column to bins and label them
#safety$incomebins <- cut(safety$incomea, breaks = breaks, labels = labels, include.lowest = TRUE)

# Convert the income_bins column to a factor
#safety$incomebins <- as.factor(safety$incomebins)

# Delete other income columns
#safety <- subset(safety, select = -c(incomeind,income))
```

```{r}
# Find the position of 'incomea'
#pos <- which(names(safety) == "incomea")

# Create a new column order
#new_order <- c(names(safety)[1:(pos-1)], "incomebins", names(safety)[pos:(length(safety)-1)])

# Rearrange the columns
#safety <- safety[, new_order]

#head(safety)
seed <- 123
```

```{r}
set.seed(seed)
trainingIndex <- createDataPartition(safety$Choice, p = 0.8, list = FALSE)

trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
```

```{r}
set.seed(seed)
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1], as.factor(trainingSet$Choice),
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
```

```{r}
set.seed(seed)
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
#importance(model)
model
```

```{r}
pred <- predict(model, testSet, type="prob")
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
```

```{r}
logloss <- function(test_set, testpredict_df) {
  # Create one-hot encoding for each choice on-the-fly
  Ch1 <- as.integer(test_set$Choice == 1)
  Ch2 <- as.integer(test_set$Choice == 2)
  Ch3 <- as.integer(test_set$Choice == 3)
  Ch4 <- as.integer(test_set$Choice == 4)
  
  # Calculate logloss using these one-hot encoded variables
  result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
                                    Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
                                    Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
                                    Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
  return(result)
}
```

```{r}
loss <- logloss(testSet, as.data.frame(pred))
loss
```
```{r}
raynard_df <- as.data.frame(pred)
raynard_df$No <- testSet$No
raynard_df <- raynard_df[c("No","Ch1","Ch2","Ch3","Ch4")]
write.csv(raynard_df, file = "./testresults_2001for_allvariables.csv", row.names = FALSE)
```


```{r}
#set all character variables as factor
trainingSet <- subset(trainingSet, select=c(Choice, Price3, Price2, Price1, agea, incomea, milesa, nighta, year, yearind, incomebins, miles, milesind ,nightind, night, pparkind, ppark, segment, segmentind, region, regionind, BU3))

testSet <- subset(testSet, select=c(Choice, Price3, Price2, Price1, agea, incomea, milesa, nighta, year, yearind, incomebins, miles, milesind ,nightind, night, pparkind, ppark, segment, segmentind, region, regionind, BU3))

fact_col <- colnames(trainingSet)[sapply(trainingSet,is.character)]

for(i in fact_col)
        set(trainingSet,j=i,value = factor(trainingSet[[i]]))

for(i in fact_col)
        set(testSet,j=i,value = factor(testSet[[i]]))
```

```{r}
levels(trainingSet$incomebins)
levels(testSet$incomebins)
```


```{r}
traintask <- makeClassifTask(data = trainingSet,target = "Choice")
testtask <- makeClassifTask(data = testSet,target = "Choice")
```

```{r}
rdesc <- makeResampleDesc("CV",iters=5L)
```

```{r}
#Random Forest without Cutoff
rf.lrn <- makeLearner("classif.randomForest", predict.type = "prob")
rf.lrn$par.vals <- list(ntree = 2001L,
                        importance=TRUE)

r <- resample(learner = rf.lrn
              ,task = traintask
              ,resampling = rdesc
              ,measures = list(acc)
              ,show.info = T,
              mtry = best.m)
```

```{r}
CVmodel <- train(rf.lrn, traintask)
```

```{r}
CVmodel
```

```{r}
CVRF <- predict(CVmodel, testtask, type="prob")
```

```{r}
CVRF
```

```{r}
CVRF <- CVRF$data[,3:6]
CVRF
```

```{r}
colnames(CVRF) <- c("Ch1", "Ch2", "Ch3", "Ch4")
CVloss <- logloss(testSet, CVRF)
CVloss
```

```{r}
getNum <- read.csv("./test1_trying.csv")

test <- subset(getNum, select = -c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))

# Convert the incomea column to bins and label them
#test$incomebins <- cut(test$incomea, breaks = breaks, labels = labels, include.lowest = TRUE)

# Convert the income_bins column to a factor
#test$incomebins <- as.factor(test$incomebins)

# Delete other income columns
#test <- subset(test, select = -c(incomeind,income))

# Find the position of 'incomea'
#pos <- which(names(test) == "incomea")

# Create a new column order
#new_order <- c(names(test)[1:(pos-1)], "incomebins", names(test)[pos:(length(test)-1)])

# Rearrange the columns
#test <- test[, new_order]

#head(safety)

```

```{r}
#test <- subset(test, select = c(Choice, Price3, Price2, Price1, agea, incomea, milesa, nighta, year, yearind, income, incomeind, miles, milesind ,nightind, night, pparkind, ppark, segment, segmentind, region, regionind, BU3))
#fact_col <- colnames(test)[sapply(test,is.character)]

#for(i in fact_col)
#       set(test,j=i,value = factor(test[[i]]))


#testClass <- makeClassifTask(data = test,target = "Choice")
#set.seed(seed)
#mtry <- tuneRF(safety[1:ncol(safety)-1], as.factor(safety$Choice),
#               stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
#best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]

#set.seed(seed)
#rf <-randomForest(as.factor(Choice)~., data = safety, mtry=best.m, importance=TRUE)

final_predict <- predict(model, test, type="prob")
#final_predict <- final_predict$data[,3:6]
colnames(final_predict) <- c("Ch1","Ch2","Ch3","Ch4")
final_predict_df <- as.data.frame(final_predict)
final_predict_df$No <- getNum$No

final_predict_df <- final_predict_df[c("No","Ch1","Ch2","Ch3","Ch4")]
```

```{r}
write.csv(final_predict_df, file = "./Joash_randForest_nobins.csv", row.names = FALSE)
```

```{r}
traintask
```
```{r}
testClass
```

