---
title: "TAE Competition Team 8 Ethan Leng Submission 2"
output: html_notebook
---

# Run data to see what we're dealing with here

```{r}
mydata <- read.csv("../input/train1.csv")
str(mydata)
summary(mydata)
head(mydata)
```

## Add a categorical column for choice
Before that we notice that our Ch1 Ch2 Ch3 Ch4 are all binary, we should combine to get an additional choice column for ease later (trust me)

Ok this gonna be nasty af but we make a new column of "Choice" by as logical
```{r}
library("dplyr")
# Change column name of choices to 1
mydata <- mydata %>% 
        rename("1" = "Ch1",
               "2" = "Ch2",
               "3" = "Ch3",
               "4" = "Ch4")
sub <- subset(mydata,select=c('1','2','3','4'))

# Make a new column which uses logical functions to get the choice
mydata$choice <- apply(sub, 1, function(x) names(x)[as.logical(as.numeric(as.character(x)))])

# Rename the columns back
mydata <- mydata %>% 
        rename("Ch1" = "1",
               "Ch2" = "2",
               "Ch3" = "3",
               "Ch4" = "4")
```



## Summary
14250 Observations
98 Observations (94 variables + 4 choices)
Each customer picks 20 choices
CC GN NS BU FA LD
BZ FC FP RP PP KA
SC TS NV MA LB AF
HU Price

Demographic Information: 

***ind means index, they are labelled in order

segment = Full-size Pickup, Midsize Car, Midsize Luxury Utility segements, Midsize Utility, Prestige Luxury Sedan, Small Car
segmentind = 1,2,3,4,5,6

year = 2000 2001 2002 2003 2004 2005 2006
yearind = 1,2,3,4,5,6,7

miles = Under 50 Miles, 51 To 100 Miles, 101 To 150 Miles, 151 To 200 Miles, 201 To 250 Miles, 251 To 300 Miles, 301 To 350 Miles, 351 To 400 Miles, Over 400 Miles
milesind = 1,2,3,4,5,6,7,8,9

night = Under 10%, 10% To 20%, 21% To 30%, 31% To 40%, 41% To 50%, 51% To 60%, 61% To 70%, 71% To 80%, 81% To 90%, 91% To 100%
nightind = 1,2,3,4,5,6,7,8,9,10

nighta

gender = Female, Male
genderind = 1 or 2

age = Under 30, 30 To 39, 40 To 49, 50 To 59, 60 & Over   
ageind = 1,2,3,4,5

ppark
pparkind

agea = actual age from 18 - 81 (64 levels)

educ = College Graduate (4 Years), Grade School, High School, Postgraduate College, Some College (1-3 Years), Trade/Vocational School
eduind = 1,2,3,4,5,6

region = MW, NE, SE, SW, W
regionind = 1,2,3,4,5

Urb = Rural/Country, Suburban, Urban/City 
Urbind = 1,2,3

income = 
incomeind = 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22,24,26,27,28
there are 25 income levels but the numbers go to 28 (yes)

incomea = actual income with 152 levels


# At this juncture we will start to play with the data
Instead of selecting all 19 tasks we can pick the first 12 as our training and anything above 12 as our validation
To add one take note that for the ageind, Female = 1 and Male = 2, if we were to use this for our model we would get fuk, so convert these to binary first. (Female = 1, Male =0)

```{r}
mydata$genderind <- as.integer(mydata$genderind == 1)
```


## Ok 1st part done now we copy the same steps as him and see what we get

First we gotta pivot the data (since each row we have, we have 4 selections made by 1 person)
- We use the 12 selections that they made as our decision variables (12/20)
- Shape of the data frame is a wide boi (where each row is an observation)
- variable indicating the choice made is defined by 'Choice'
```{r}
library(mlogit)
training <- dfidx(subset(mydata, Task<=12), shape="wide", sep="", choice = "choice", varying = c(4:83), idx = c("No", "Case"))
head(training)
```

## Time to make model

```{r}
model1 <- mlogit(choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data=training)
summary(model1)
```
## Clean up the shitty vars
```{r}
model2 <- mlogit(choice~CC+GN+NS+BU+SC+FA+LD+RP+PP+KA+TS+MA+LB+HU+Price-1, data=training)
summary(model2)
```

## Check accuracy against itself
```{r}
ActualChoice <- subset(mydata, Task<=12)[,"choice"]
pred_training <- predict(model2, newdata=training)
PredictedChoice <- apply(pred_training,1,which.max)
traintable=table(PredictedChoice, ActualChoice)
traintable
acc<-sum(diag(traintable))/sum(traintable)
acc
```
  
## Try using the training model to predict the validation set
```{r}
validation <- dfidx(subset(mydata, Task>12), shape="wide", sep="", choice = "choice", varying = c(4:83), idx = c("No", "Case"))
head(validation)
```
```{r}
valid_actual <- subset(mydata, Task>12)[,"choice"]
pred_valid <- predict(model2, newdata=validation)
PredictedChoice <- apply(pred_valid,1,which.max)
validtable = table(PredictedChoice, valid_actual)
validtable
acc<-sum(diag(validtable))/sum(validtable)
acc
```


## Calculating the log loss value

$$
\text{LogLoss} = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^{m=4}y_{ij}\cdot\text{ln}(p_{ij}), 
$$
Take note that $y_{ij}$ is a binary variable. $m$ are the number of choices from 1-4.
So for each row, we multiply the row of actual choices with the natural log of probabilities then sum them and divide by -1/n.
Example)
$$
\text{(0 0 0 1)}\cdot \text{ln} \left(\text{(0.1 0.3 0.2 0.5)}^{\text{transpose}}\right)
$$


## Calculating log loss for my training set
```{r}
#P
actual <- subset(mydata,Task<=12,select=c('Ch1','Ch2','Ch3','Ch4'))
#actual
logloss <- -(1/nrow(actual))*sum(actual*log(pred_training))
logloss
```

## Calculating log loss for my validation set
```{r}
#P
actual <- subset(mydata,Task>12,select=c('Ch1','Ch2','Ch3','Ch4'))
#actual
logloss <- -(1/nrow(actual))*sum(actual*log(pred_valid))
logloss
```


# K so those 2 models were dog shit
Let's introduce the rest of the variables into this model.
Maybe the demographics of the applicants actually make a difference?

We will use the "ind" variables as they are 
Take note we changed the genderind earlier to a binary variable

Are there any more variables that we should be changing?
Ordinal data (ranks)
yeardind
milesind
nightind
pparkind
ageind

Nominal data (unordered)
genderind (converted to binary)
segmentind (SHOULD convert to binary by adding dummy vars but lets not do that now)


The | after the Price variable basically states everything after is a generic variable
https://stats.stackexchange.com/questions/32585/singularity-issues-in-multinomial-model-using-r
idk wtf that means but makes sense
```{r}
model3 <- mlogit(choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price|segmentind+yearind+milesind+nightind+pparkind+genderind+ageind-1, data=training)
summary(model3)
```
## Drop useless vars (JK actually trash)
```{r}
model4 <- mlogit(choice~CC+BU+Price|segmentind+yearind+milesind+nightind+ageind+genderind-1, data=training)
summary(model4)
```


## Model 3 Check accuracy against itself
```{r}
ActualChoice <- subset(mydata, Task<=12)[,"choice"]
pred_training <- predict(model3, newdata=training)
PredictedChoice <- apply(pred_training,1,which.max)
traintable=table(PredictedChoice, ActualChoice)
traintable
acc<-sum(diag(traintable))/sum(traintable)
acc
```


## Model 3 Calculating log loss for my training set
```{r}
#P
actual <- subset(mydata,Task<=12,select=c('Ch1','Ch2','Ch3','Ch4'))
#actual
logloss <- -(1/nrow(actual))*sum(actual*log(pred_training))
logloss
```

## Model 3 Check accuracy against validation
```{r}
valid_actual <- subset(mydata, Task>12)[,"choice"]
pred_valid <- predict(model3, newdata=validation)
PredictedChoice <- apply(pred_valid,1,which.max)
validtable = table(PredictedChoice, valid_actual)
validtable
acc<-sum(diag(validtable))/sum(validtable)
acc
```

## Calculating log loss for my validation set
```{r}
#P
actual <- subset(mydata,Task>12,select=c('Ch1','Ch2','Ch3','Ch4'))
#actual
logloss <- -(1/nrow(actual))*sum(actual*log(pred_valid))
logloss
```


## Predicting the test set now
```{r}
testdata <- read.csv("../input/test1.csv")

# remove these 4 columns as we there are NA columns and cause problems later one (we also dont use them)
test <- subset(testdata, select= -c(Ch1,Ch2,Ch3,Ch4))

#we create a new column choice because the predict() function sometimes searches for it, so add a 0 column 4 fun
test$choice <- 0
head(test)
```
## Formatting the data for the test set
```{r}
testing <- dfidx(subset(test, Task<=19), shape="wide", sep="", varying = c(4:83), idx = c("No", "Case"))
head(testing)
```


## Predicting
```{r}
test_pred <- predict(model3, newdata=testing)
head(test_pred)
```

## Exporting
```{r}
test_pred <- as.data.frame(test_pred) %>% 
        rename("Ch1" = "1",
               "Ch2" = "2",
               "Ch3" = "3",
               "Ch4" = "4")
```

