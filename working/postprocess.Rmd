---
title: "Postprocessing"
output: html_notebook
---
# Load the required library
```{r}
rm(list=ls())
library(dplyr)
library(purrr)
library(ggplot2)
```

# Read in the prediction data
```{r}
test1 <- read.csv("../input/testresults_2001for_allvariables.csv")
test2 <- read.csv("../input/testresults_xgb3.csv")
test3 <- read.csv("../input/testresults_mlogitM3.csv")
pred1 <- read.csv("../output/Rforest_2001_trees.csv")
pred2 <- read.csv("../output/submission_xgb3.csv")
pred3 <- read.csv("../output/submission_M3.csv")
```

```{r}
# Soft voting function
soft_voting <- function(pred1, pred2, pred3=NULL, weights=c(0.5, 0.5, 0)) {
  # Ensure that the weights sum up to 1
  if (sum(weights) != 1) {
    stop("The weights do not sum up to 1.")
  }

  # Remove 'Choice' from pred2 and pred3 (if present)
  pred2 <- pred2[ , !(names(pred2) %in% "Choice")]
  if (!is.null(pred3)) {
    pred3 <- pred3[ , !(names(pred3) %in% "Choice")]
  }

  if (!is.null(pred3)) {
    # Merge all predictions based on 'No'
    pred <- merge(merge(pred1, pred2, by = "No", suffixes = c("_1", "_2")), pred3, by = "No")
    
    # Soft voting
    pred <- pred %>%
      mutate(
        Ch1 = weights[1] * Ch1_1 + weights[2] * Ch1_2 + weights[3] * Ch1,
        Ch2 = weights[1] * Ch2_1 + weights[2] * Ch2_2 + weights[3] * Ch2,
        Ch3 = weights[1] * Ch3_1 + weights[2] * Ch3_2 + weights[3] * Ch3,
        Ch4 = weights[1] * Ch4_1 + weights[2] * Ch4_2 + weights[3] * Ch4
      )
  } else {
    # Merge pred1 and pred2 based on 'No'
    pred <- merge(pred1, pred2, by = "No", suffixes = c("_1", "_2"))

    # Soft voting
    pred <- pred %>%
      mutate(
        Ch1 = weights[1] * Ch1_1 + weights[2] * Ch1_2,
        Ch2 = weights[1] * Ch2_1 + weights[2] * Ch2_2,
        Ch3 = weights[1] * Ch3_1 + weights[2] * Ch3_2,
        Ch4 = weights[1] * Ch4_1 + weights[2] * Ch4_2
      )
  }

  # Remove the columns from individual models
  cols_to_drop <- grep("_", names(pred), value = TRUE)
  pred <- pred %>% select(-cols_to_drop)
  
  return(pred)
}

# Function to adjust and normalize probabilities
adjust_and_normalize <- function(df, min_proportion=0.5) {
  
  # List of column names containing probabilities
  prob_cols <- c("Ch1", "Ch2", "Ch3", "Ch4")
  
  # Convert the dataframe to a matrix for easier row-wise operations
  prob_matrix <- as.matrix(df[prob_cols])
  
  # For each row, find the min and max indices
  min_indices <- apply(prob_matrix, 1, which.min)
  max_indices <- apply(prob_matrix, 1, which.max)
  
  # Adjust the min and max values for each row
  for(i in seq_len(nrow(prob_matrix))) {
    # Calculate how much of the min to give to the max
    transfer_amount <- prob_matrix[i, min_indices[i]] * min_proportion
    
    # Subtract the transfer amount from the min
    prob_matrix[i, min_indices[i]] <- prob_matrix[i, min_indices[i]] - transfer_amount
    
    # Add the transfer amount to the max
    prob_matrix[i, max_indices[i]] <- prob_matrix[i, max_indices[i]] + transfer_amount
  }
  
  # Normalize probabilities so they sum to 1
  prob_matrix <- prob_matrix / rowSums(prob_matrix)
  
  # Replace the old probability columns in the dataframe
  df[prob_cols] <- prob_matrix
  
  return(df)
}

logloss <- function(testpredict_df) {
  # Create one-hot encoding for each choice on-the-fly
  Ch1 <- as.integer(testpredict_df$Choice == 1)
  Ch2 <- as.integer(testpredict_df$Choice == 2)
  Ch3 <- as.integer(testpredict_df$Choice == 3)
  Ch4 <- as.integer(testpredict_df$Choice == 4)
  
  # Calculate logloss using these one-hot encoded variables
  result <- -1/nrow(testpredict_df) * sum(Ch1 * log(testpredict_df$Ch1) +
                                    Ch2 * log(testpredict_df$Ch2) +
                                    Ch3 * log(testpredict_df$Ch3) +
                                    Ch4 * log(testpredict_df$Ch4))
  return(result)
}
```

```{r}
# With two predictions
vote2 <- soft_voting(test1, test2, weights = c(0.6, 0.4))
loss2 <- logloss(vote2)
loss2

# With three predictions
vote3 <- soft_voting(test1, test2, test3, weights = c(0.5, 0.35, 0.15))
loss3 <- logloss(vote3)
loss3
```

```{r}
vote2
vote3
```


```{r}
# Initialize variables to store the minimum loss and corresponding weight
min_loss <- Inf
optimal_weight <- NULL

# Initialize the dataframe to store the weight and corresponding loss
df_loss <- data.frame(weight1 = numeric(), loss = numeric())

# Iterate over the range of weights
for(weight1 in seq(0, 1, by=0.01)){
  # Get soft voting result
  result <- soft_voting(test1, test2, weights=c(weight1, 1-weight1))
  
  # Calculate the logloss
  loss <- logloss(result)
  print(c(weight1,loss))
  # Check if this loss is less than the current minimum loss
  if(loss < min_loss){
    # If so, update the minimum loss and the corresponding weight
    min_loss <- loss
    optimal_weight <- weight1
  }
  
  # Append the weight and corresponding loss to the dataframe
  df_loss <- rbind(df_loss, data.frame(weight1 = weight1, loss = loss))
}

# Print the optimal weight and corresponding loss
cat("Optimal weight1: ", optimal_weight, "\n")
cat("Minimum loss: ", min_loss, "\n")

# Plot the loss against weight
ggplot(df_loss, aes(x=weight1, y=loss)) +
  geom_line() +
  labs(x="Weight1", y="Loss", title="Loss vs Weight1")
```


```{r}
# Apply the function
final2 <- adjust_and_normalize(vote2, 0.5)
loss2 <- logloss(final2)
loss2
final3 <- adjust_and_normalize(vote3, 0.5)
loss3 <- logloss(final3)
loss3
```

# Exporting
```{r}
# # With two predictions
# vote2 <- soft_voting(pred1, pred2, weights = c(0.40, 0.60))

# With three predictions
vote3 <- soft_voting(pred1, pred2, pred3, weights = c(0.45, 0.35, 0.20))
```

```{r}
min(subset(result, select=-c(No)))
max(subset(result, select=-c(No)))
min(subset(final, select=-c(No)))
max(subset(final, select=-c(No)))
```

## Export
```{r}
write.csv(vote3, file = "../output/ensemble_453520.csv", row.names = FALSE)
```

